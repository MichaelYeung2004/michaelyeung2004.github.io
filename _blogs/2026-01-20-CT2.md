---
layout: blog
title: '利用链式法则和贝叶斯定理比较概率分布'
date: 2026-01-20
tags:
  - deep learning
  - paper
---

Exploiting Chain Rule and Bayes’ Theorem to Compare Probability Distributions

对于Comparing Probability Distributions with Conditional Transport，我们将原来的Navigator思想，**升华到了一个更根本的概率论高度**，直接与**概率链式法则 (Chain Rule)** 和 **贝叶斯定理 (Bayes' Theorem)** 这两大基石联系起来。

## 理论框架——链式法则与贝叶斯定理

**如何用概率论的基本法则来构建两个分布之间的关联？**

### 1. 概率链式法则 (Chain Rule)

概率链式法则是概率论中最基本的公式之一，它告诉我们如何分解一个联合概率分布。对于两个随机变量 $x$ 和 $y$，它们的联合分布 $\pi(x, y)$ 可以用两种方式分解：

1.  **方式一（前向分解）**：$\pi(x, y) = p_x(x) \times \pi(y\mid x)$
2.  **方式二（后向分解）**：$\pi(x, y) = p_y(y) \times \pi(x \mid y)$

我们想要构建一个连接真实分布 $p_x(x)$ 和生成分布 $p_y(y)$ 的“桥梁”，这个桥梁就是联合分布 $\pi(x, y)$。链式法则告诉我们有两种建桥的方式：

*   **前向建桥（Forward CT）**：我们先从真实世界中抽取一个样本 $x$（遵循 $p_x(x)$），然后**基于这个 $x$**，我们再通过一个条件分布 $\pi(y\mid x)$ 来选择一个与之匹配的生成样本 $y$。
*   **后向建桥（Backward CT）**：我们先从生成器中抽取一个样本 $y$（遵循 $p_y(y)$），然后**基于这个 $y$**，我们再通过一个条件分布 $\pi(x\mid y)$ 来寻找一个与之匹配的真实样本 $x$。

### 2. 贝叶斯定理 (Bayes' Theorem)

现在的问题是，那个条件分布 $\pi(y\mid x)$（我们之前称之为Navigator）到底应该长什么样？它不能是随便一个分布，它必须同时体现出真实样本 $x$ 和生成分布 $p_y(y)$ 的信息。

作者选择用贝叶斯定理来定义它。

我们先复习一下贝叶斯定理的经典形式：

$$ \text{后验概率} \propto \text{似然} \times \text{先验概率} $$

$$ P(\text{原因}\mid \text{结果}) \propto P(\text{结果}\mid \text{原因}) \times P(\text{原因}) $$

对于前向传输 $\pi(y\mid x)$，我们可以这样理解：

*   **问题**：给定一个**已观测到的真实数据 $x$**（结果），我们想要推断它最可能匹配的**生成数据 $y$**（原因）的概率分布。
*   **先验概率 (Prior)**：在我们看到任何真实数据 $x$ 之前，我们对 $y$ 的“先入为主”的看法是什么？很简单，我们认为 $y$ 应该来自于生成器的分布，所以**先验就是 $p_y(y)$**。这代表了生成器本身生成各个 $y$ 的“热门程度”。
*   **似然 (Likelihood)**：如果一个生成数据真的是 $y$，那么它和我们观测到的真实数据 $x$ “长得像”的可能性有多大？作者将这个“可能性”定义为一个与距离相关的函数，即**似然 $\propto e^{-d_\phi(x, y)}$**。这里的 $d_\phi(x,y)$ 依然是那个可学习的距离函数。两个点离得越近，似然就越大。
*   **后验概率 (Posterior)**：结合了先验和似然之后，我们得到的更新后的概率分布，就是我们的条件分布 $\pi(y\mid x)$。

把它们整合起来，就得到了论文中的**公式(1)**：
$$ \pi_\gamma(y\mid x) = \frac{e^{-d_\phi(x, y)} p_y(y)}{Q(x)}, \quad \text{其中 } Q(x) = \int e^{-d_\phi(x, y')} p_y(y') dy' $$

Q(x)是归一化常数

**这个公式现在有了贝叶斯解释：**
给定一个真实样本 $x$，它将被传输到一个生成样本 $y$ 的概率（后验），正比于 “$y$ 本身作为生成样本的普遍性（先验）” 与 “$y$ 和 $x$ 在特征上的相似性（似然）” 的乘积。

## 方法的核心机制

有了这个新框架，我们再来看CT的成本函数。

### 1. 前向与后向CT成本

**前向CT成本 (公式2)**：
$$ C(X \to Y) = \mathbb{E}_{x \sim p_x(x)} \mathbb{E}_{y \sim \pi(y\mid x)} [c(x,y)] $$
它的含义是：我们按照**前向建桥**方式 $\pi(x, y) = p_x(x) \pi(y \mid  x)$ 构建的联合分布，计算在该分布下，点对成本 $c(x,y)$ 的期望值。

*   **与Mode-Covering的联系**：论文在这里明确指出了它和KL散度的深刻联系。最小化这个成本，其效果类似于最小化 **KL散度 $KL(p_x \vert \vert p_y)$**。KL散度的性质是，只要某个地方 $p_x(x) > 0$，为了让KL散度不为无穷大，就必须有 $p_y(x) > 0$。也就是说，**真实数据存在的地方，生成数据也必须存在**。这正是“模式覆盖”的数学本质！

**后向CT成本 (公式4)**：
$$ C(X \leftarrow Y) = \mathbb{E}_{y \sim p_y(y)} \mathbb{E}_{x \sim \pi(x|y)} [c(x,y)] $$
它的含义是：我们按照**后向建桥**方式 $\pi(x, y) = p_y(y) \pi(x \mid y)$ 构建的联合分布，计算成本的期望值。

*   **与模式搜寻 (Mode-Seeking) 的联系**：同样，最小化这个成本，其效果类似于最小化**反向KL散度 $KL(p_y || p_x)$**。反向KL散度允许在某些地方 $p_y(x)=0$ 即使 $p_x(x)>0$。也就是说，**生成的数据可以只专注于真实数据中密度最高、最典型的区域**，而忽略那些边缘、不典型的区域。这正是“模式搜寻”和可能导致“模式崩塌”的数学本质。

### 2. 平衡参数 $\rho$ (rho)

**总CT成本 (公式5)**：
$$ C_\rho(X, Y) := \rho C(X \to Y) + (1-\rho)C(X \leftarrow Y) $$

*   $\rho \in$ 是一个可以调节的**超参数**。
*   当 $\rho=1$ 时，模型只关心**模式覆盖**（可能会生成模糊图像）。
*   当 $\rho=0$ 时，模型只关心**模式搜寻**（很可能发生模式崩塌）。
*   当 $\rho=0.5$ 时（默认值），模型在两者之间取得平衡。

### 3. 基于共轭性的解析条件分布

在进入充满不确定性的现实世界（用样本近似）之前，作者先创建了一个理想化的沙盒。在这个沙盒里，所有的问题都有**解析解 (analytic solution)**，也就是说，我们可以用漂亮的数学公式直接把所有东西算出来，而不需要任何近似。

这个沙盒的作用是：
1.  **验证理论**：在一个可以精确计算的环境中，验证我们之前关于模式覆盖/搜寻的猜想是否正确。
2.  **提供洞察**：观察在这个理想环境中，各个参数是如何相互作用、共同达到最优解的，从而为理解更复杂的情况提供宝贵的直觉。

#### 1. 共轭性

要获得解析解，我们需要一个特殊的数学性质，叫做**共轭性 (Conjugacy)**。在贝叶斯统计中，“共轭”指的是**先验分布 (Prior)** 和**后验分布 (Posterior)** 属于同一个概率分布家族。

回忆一下我们的前向Navigator公式（贝叶斯公式）：
$$ \pi(y|x) \propto \underbrace{p_y(y)}_{\text{先验}} \times \underbrace{e^{-d_\phi(x, y)}}_{\text{似然}} $$
如果我们的先验分布 $p_y(y)$ 和似然函数 $e^{-d_\phi(x, y)}$ 的形式“很搭”，使得它们的乘积（后验分布 $\pi(y|x)$）依然保持着和先验 $p_y(y)$ 类似的数学形式，我们就称它们是**共轭**的。

**最经典的共轭例子就是正态分布（高斯分布）。** 如果先验是正态分布，似然函数也是正态分布的形式，那么后验分布也必然是正态分布。

接下来，利用正态分布的共轭性，得到：

**公式(6): 单变量正态分布的例子**
*   **源分布 (真实)**：$p_x(x) = \mathcal{N}(0, 1)$，一个标准正态分布。
*   **目标分布 (生成)**：$p_y(y) = \mathcal{N}(0, e^\theta)$，一个均值为 0，方差为 $e^\theta$ 的正态分布。
*   **距离函数**：$d_\phi(x, y) = \frac{(x-y)^2}{2e^\phi}$。
*   **成本函数**：$c(x, y) = (x-y)^2$。

**为什么这个设置是“理想”的？**
因为这里的似然函数形式 $e^{-d_\phi(x, y)} = e^{-\frac{(x-y)^2}{2e^\phi}}$ 正好也是一个（未归一化的）**正态分布**的形式,而我们的先验 $p_y(y)$ 也是正态分布。正态乘以正态，结果还是正态。

因此，在这个设置下，前向导航器 $\pi(y\mid x)$ 和后向导航器 $\pi(x\mid y)$ 都可以被精确地计算出来，它们的结果也都是正态分布。论文在附录C中给出了详细的推导，最终得到：

*   **前向导航器**：$\pi(y\mid x) = \mathcal{N}(\sigma(\phi-\theta)x, \sigma(\phi-\theta)e^\phi)$
*   **后向导航器**：$\pi(x\mid y) = \mathcal{N}(\sigma(-\phi)y, \sigma(\phi))$
*   **前向成本**：$C(X \to Y) = \sigma(\phi-\theta)(e^\theta + \sigma(\phi-\theta))$
*   **后向成本**：$C(X \leftarrow Y) = \sigma(\phi)(1 + \sigma(\phi)e^\theta)$

这里的 $\sigma(\alpha) = 1/(1+e^{-\alpha})$ 是**Sigmoid函数**。你不需要记住这些复杂的公式，关键在于理解：**在这个沙盒里，所有东西都有明确的数学表达式。**

#### **3. 新指标：用KL散度之差量化“行为倾向”**

在这个可以精确计算的沙盒里，作者引入了一个新的指标 $D(X, Y)$ 来**定量地**描述目标分布 $p_y$ 相对于源分布 $p_x$ 的行为倾向。

**定义: $D(X, Y)$**
$$ D(X, Y) = KL(p_x || p_y) - KL(p_y || p_x) $$
即**前向KL散度**与**反向KL散度**之差。

*   **如果 $D(X, Y) < 0$**：意味着 $KL(p_x || p_y) < KL(p_y || p_x)$。这表明，从KL散度的角度看，“让$p_y$覆盖$p_x$”比“让$p_y$被$p_x$覆盖”要“容易”得多。这对应于**模式覆盖 (mode-covering)** 行为。
*   **如果 $D(X, Y) > 0$**：意味着 $KL(p_y || p_x)$ 更小。这对应于**模式搜寻 (mode-seeking)** 行为。

在我们的沙盒例子中，这个 $D(X, Y)$ 也可以被精确计算出来：
$$ D(X, Y) = \dots = \sinh(\theta) - \theta $$
其中 $\sinh(\theta)$ 是双曲正弦函数。（注意：论文v5中写的是$\theta - \sinh(\theta)$，但根据KL散度的性质和图1的分析，应该是这个形式才能保证 $\theta<0$ 时 mode-seeking）。
*   当 $\theta > 0$（生成分布的方差大于真实分布）时，$D(X, Y) > 0$，表现为**模式覆盖**。
*   当 $\theta < 0$（生成分布的方差小于真实分布）时，$D(X, Y) < 0$，表现为**模式搜寻**。

这为我们提供了一个漂亮的理论工具，来连接模型的参数（$\theta$）和它所表现出的宏观行为。

#### **4. 沙盒中的实验验证 (图1)**

现在我们终于可以完全理解**图1**了。这张图正是在这个理想的沙盒中绘制的。
*   **左图（演化图）**：展示了当使用总CT成本（$\rho=0.5$）进行梯度下降时，参数 $\theta$ 和 $\phi$ 会沿着红色轨迹线（我们在上一封邮件的图解中看到的）顺利地收敛到最优解 $(0, 0)$。这证明了CT损失函数在这个理想环境下是有效的。
*   **右侧四张图（成本曲线）**：这四张图是本节的精髓。它们展示了当导航器参数 $\phi$ 已经被优化得很好时（从 $e^{-\phi}=1$ 到 $e^{-\phi}=20$，即 $\phi$ 从0逐渐变小），前向、后向和总成本随生成器参数 $\theta$ 变化的曲线。
    *   **前向成本曲线（蓝色）**：最低点总是在 $\theta > 0$ 的地方。这**再次验证**了前向成本驱动**模式覆盖**（倾向于生成更大方差的分布）。
    *   **后向成本曲线（橙色）**：最低点总是在 $\theta < 0$ 的地方。这**再次验证**了后向成本驱动**模式搜寻**（倾向于生成更小方差的分布，即更“自信”的、窄的分布）。
    *   **总成本曲线（绿色）**：最低点总是在 $\theta=0$ 附近。它完美地平衡了前两者的“拉力”，将最优解定位在正确的位置。
    *   **一个更深的洞察**：随着导航器越来越优（$\phi$ 变小），总成本曲线在 $\theta=0$ 处的“山谷”变得越来越**尖锐和陡峭**。这意味着一个好的导航器能够为生成器提供**更强大、更明确的梯度信号**，帮助它更快地收敛到最优解。这解释了论文中的一句话：“导航器参数 $\phi$ 主要扮演了**辅助学习 $\theta$** 的角色”。

---
**第二部分补充总结**

1.  **理论沙盒**：作者通过一个基于**正态分布共轭性**的理想化例子，构建了一个所有量都可以被**精确计算**的环境。
2.  **新指标 $D(X,Y)$**：引入了$KL(p_x||p_y) - KL(p_y||p_x)$ 作为量化模式覆盖/搜寻行为的指标。
3.  **理论验证**：在这个沙盒中，通过绘制精确的成本曲线，**从理论上严格验证**了前向成本驱动“模式覆盖”，后向成本驱动“模式搜寻”，而总CT成本能够实现完美平衡。
4.  **导航器的作用**：揭示了导航器的优化能够让生成器的学习“地形”变得更好，从而加速收敛。

这个小节虽然不涉及最终的算法实现，但它为整个CT框架的合理性提供了强有力的理论支撑和深刻的直觉洞察。现在，我们已经做好了万全的理论准备，可以进入下一部分，看看如何在现实世界中实现和应用CT了。
