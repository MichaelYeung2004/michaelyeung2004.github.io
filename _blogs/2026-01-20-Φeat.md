---
layout: blog
title: 'Φeat: 基于物理的特征表示'
date: 2026-01-20
tags:
  - deep learning
  - Text-to-Video Generation
  - paper

---

NewtonGen关心的是物体**如何运动**，而Φeat关心的是物体**由什么构成**。

## Background

现在的AI视觉模型（比如CLIP, DINO等）就像一个知识渊博但有点脸盲的图书管理员。你给它看一张照片，里面有一辆红色的金属跑车和一只红色的塑料玩具鸭。

你指着跑车的金属车门问它：“**这张图里，还有哪些东西和这个很像？**”

这个“图书管理员”AI可能会：
*   **语义关联 (Semantic Association)**：它会把跑车的其他部分，比如轮胎、车窗都找出来。因为它知道这些都属于“车”这个物体。
*   **颜色关联 (Color Association)**：它可能会把那只红色的塑料玩具鸭也圈出来，因为它俩都是红色的。

但你真正想问的可能是：“**这张图里，还有哪些东西是‘金属’的？**” 这个AI很可能就答不上来了。它可能无法理解，一把金属钥匙、一个不锈钢锅和这扇车门，在“材质”这个层面上是“同类”。

**这就是论文要解决的核心问题：**
现有的自监督学习模型非常擅长学习**语义 (Semantics)**，也就是“**这个东西是什么？**”（是车、是鸭子）。但它们很难分清语义和物理属性，导致它们不理解**物理特性 (Physical Properties)**，也就是“**这个东西是什么构成的？**”（是金属、是塑料、是木头）。

这种能力的缺失，在很多领域是致命的，比如：
*   **机器人**：机器人需要知道一个物体是软是硬，是光滑还是粗糙，才能决定用多大的力去抓取。
*   **自动驾驶**：需要判断路面是干燥沥青、是湿滑路面还是结冰，这都和材质有关。
*   **三维重建/AR**：要让虚拟物体真实地融入现实场景，就必须知道现实物体的材质，才能正确模拟光线反射。

## Φeat的核心思想

### 强迫AI学会“透过现象看本质”

为了教会AI理解材质，Φeat提出了一个极其巧妙的训练策略。这个策略的核心思想是改变“什么东西算作相似”的定义。

*   **传统AI的训练方式 (Photometric Augmentation)**：
    *   拿一张猫的照片。
    *   对这张照片进行各种“化妆”：裁剪一下、旋转一下、调亮一点、变个色调。
    *   然后告诉模型：“听着，所有这些‘化了妆’的照片，**本质上都是同一张图**，你要学会认出它们是同类。”
    *   这种方法教会了模型忽略光照、角度等表面变化，去识别“猫”这个核心语义。

*   **Φeat的训练方式 (Physical Augmentation)**：
    *   Φeat说：“我们来玩个更高级的游戏。”
    *   它首先获取一种**数字化的材质**，比如“橡木”的材质数据（包含了颜色、纹理、粗糙度、反射率等信息）。
    *   然后，它用计算机图形学技术，将这种“橡木”材质分别渲染（“贴”）到**完全不同的物体上**，并且使用**完全不同的光照环境**。
    *   于是，它得到了两张看起来毫不相干的图片：
        1.  一张是**阳光下的一把橡木椅子**。
        2.  另一张是**室内灯光下的一个橡木球**。
    *   然后Φeat告诉模型：“听着，这两张图，一张是椅子，一张是球，形状、光照、背景都不同，但你要学会认识到，它们在**物理本质上是同类**，因为它们都是‘橡木’！”

通过进行成千上万次这样的训练，模型的大脑就会被强行“改造”。它会逐渐意识到，不能只看物体的形状（椅子 vs. 球）或者场景（室外 vs. 室内），而必须学会**提取出那种独立于形状和光照之外的、更为根本的“材质”特征**。

### 训练数据

为了实现这种“物理增强”的训练，Φeat需要海量的、有精确材质标签的数据。在现实世界中收集这样的数据是不可能的。

因此，作者采用了**完全基于合成数据**的方法：
1.  他们使用了一个庞大的专业材质库（Adobe Substance 3D Assets），里面有上万种定义好的数字材质。
2.  他们创建了一系列3D模型作为“衣架”。
3.  他们使用多种HDRI环境贴图作为“灯光”。
4.  通过一个自动化流程，将不同的材质随机“穿”在不同的“衣架”上，并用不同的“灯光”去“拍摄”，最终生成了约一百万张高质量的渲染图片。

这些合成图片是Φeat训练的基石，因为它们提供了一个完美的实验环境：材质这个变量可以被精确控制，而其他所有变量（形状、光照、视角）都可以尽情地改变。



## Φeat的训练流程与核心技术

Φeat的训练框架是在一个非常成功的自监督学习模型 **DINO** 的基础上改进而来的。因此，要理解Φeat，我们首先要简单了解一下DINO的“师徒”模式。

### DINO的“师徒传授”模式 (Teacher-Student)

DINO框架里有两个模型，一个叫**老师 (Teacher)**，一个叫**学生 (Student)**。

*   **学生 (Student)**：是一个正常的、需要通过训练不断学习和更新参数的模型。
*   **老师 (Teacher)**：它的网络结构和学生**一模一样**，但它的参数**不是**通过常规的梯度下降来更新的。相反，它的参数是由“学生”的参数经过**指数移动平均 (Exponential Moving Average, EMA)** 缓慢更新而来。你可以把它想象成一个“更稳定、更有经验”版本的学生。
*   **学习过程**：
    1.  给“师徒”二人看同一张图片的不同“化妆”版本（比如一张裁剪过的，一张调亮过的）。
    2.  学生需要努力地去预测老师对于这张图片的“看法”（即老师网络输出的特征）。
    3.  通过不断缩小自己和老师“看法”的差距，学生就能学到对各种“化妆”都不变的稳定特征。

Φeat沿用了这种强大的师徒模式，但把“化妆”的方式从简单的图像处理，换成了我们上次讲的“物理增强”。

### Φeat的训练流程详解

![image-20260120100754085](https://michaelyeung2004.github.io/assets/images/photos/image-20260120100754085.png)

1.  **数据准备 (Data Preparation)**：
    *   首先，程序会随机选择**一种材质**，比如“拉丝青铜”。
    *   然后，用这种材质渲染**两张完全不同的图片**。例如：
        *   **视图1 (View 1)**：阳光下一个“拉丝青铜”做的水龙头。
        *   **视图2 (View 2)**：室内灯光下一个“拉丝青铜”做的雕塑。
    *   这两张图就是我们这次训练的核心输入，它们共享物理本质，但外观迥异。

2.  **多视角裁剪 (Multi-crop Strategy)**：
    *   对于视图1和视图2，分别进行“多视角裁剪”，得到：
        *   **2个全局视图 (Global crops)**：裁剪尺寸较大，保留了图像的整体结构。
        *   **多个局部视图 (Local crops)**：裁剪尺寸较小，关注材质的细节纹理。
    *   这样做的好处是让模型既能看到全局信息，又能关注局部细节。

3.  **师徒网络处理**：
    *   **学生网络 (Student)**：接收**所有的**裁剪视图（全局和局部）。
    *   **老师网络 (Teacher)**：为了节省计算量，它**只接收全局视图**。

4.  **计算损失函数 (The Core Objectives)**
    Φeat的强大之处在于它组合了多个损失函数，从不同角度来约束模型的学习。我们来逐一解析这些损失函数，它们是Φeat成功的关键。

    *   **$L_{image}$ (图像级损失)**：
        
        * **目标**：让学生对一张图片（比如“青铜水龙头”）的看法，和老师对同一张图片的看法保持一致。这是**DINO的经典损失**，确保模型学习到对裁剪、缩放等操作的不变性。
        
        *   **公式(1)**: $$L_{image} = -\frac{1}{\vert V_t \vert \vert V_s \vert} \sum_{v_t \in V_t} \sum_{v_s \in V_s} \sum_{k=1}^{K} q_k(v_t) \log p_k(v_s)$$
            
            + **直观思想：**
              想象一个班级有K个兴趣小组（比如美术组、音乐组等）。老师（Teacher）和学生（Student）分别拿到一张图片（比如“猫”），他们需要判断这张图应该属于哪个兴趣小组。老师更有经验，判断得更准。学生的目标就是让自己的判断结果和老师的尽可能一样。
            
            + **符号拆解：**
            
              *   $v_s$: 代表**学生 (Student)** 网络看到的一个视图（view）。视图可以是一张图片的全局裁剪或局部裁剪。
            
              *   $v_t$: 代表**老师 (Teacher)** 网络看到的**同源**的一个视图。
            
              *   $V_s, V_t$: 分别代表学生和老师看到的所有视图的集合。
            
              *   $K$: 一个预先设定的**类别数量**（比如1000）。你可以把它理解为上面例子中的“兴趣小组”数量。这些类别是模型自己学习的，没有具体的人工标签。
            
              *   $p(v_s)$: 是一个K维的**概率分布向量**，由**学生**网络输出。$p_k(v_s)$ 是这个向量的第k个元素，代表学生认为视图 $v_s$ 属于第k类的概率有多大。这个概率是通过Softmax函数计算出来的。
            
              *   $q(v_t)$: 同样是一个K维的**概率分布向量**，由**老师**网络输出。$q_k(v_t)$ 代表老师认为视图 $v_t$ 属于第k类的概率。它被视为“标准答案”或“软标签”。
            
              *   $\log p_k(v_s)$: 取学生输出概率的对数。
            
              *   $\sum_{k=1}^{K} q_k(v_t) \log p_k(v_s)$: 这是计算**交叉熵 (Cross-Entropy)** 的核心部分。它衡量了学生概率分布 $p$ 和老师概率分布 $q$ 之间的“距离”或“差异”。如果两个分布完全相同，这个值最大（损失最小）；如果差异很大，这个值就变得很负（损失很大）。
            
              *   $-\sum ...$: 前面加一个负号，就把最大化相似度的问题，变成了我们熟悉的**最小化损失 (Loss)** 的问题。
            
              *   $\frac{1}{\vert V_t\vert \vert V_s\vert} \sum_{v_t \in V_t} \sum_{v_s \in V_s} ...$: 对所有师生视图对的损失进行求和，然后取平均。
            
            
            **总结：** $L_{image}$ 通过最小化师生输出概率分布的交叉熵，强迫学生网络学习到一个和老师网络相似的特征提取能力。
        
    *   **$L_{iBOT}$ (图像块级损失)**：
        
        *   **目标**：这个损失关注更细节的层面。它会随机地**遮住 (mask)** 学生输入图片的一部分图像块（patch），然后要求学生去**预测**老师网络看到的、**未被遮住的**对应图像块的特征。
        *   **公式(2)**: $L_{iBOT} = \frac{1}{M} \sum_{m=1}^{M} \vert h_{\theta}(p'_m) - p_m \vert^2$
            *   **直观思想：**
                把一张图片切成很多小方块（patches）。随机遮住学生面前的几个小方块，然后让他看老师面前完整的图片。学生需要根据自己看到的未被遮挡的上下文，去猜测那些被遮住的方块“长什么样”（在特征层面）。
            *   **符号拆解：**
                *   $m$: 代表一个被遮住的图像块的索引。
                *   $M$: 被遮住的图像块的总数。
                *   $p_m$: **老师**网络输出的、对应第m个位置的**图像块特征向量**。这是学生需要预测的“标准答案”。
                *   $p'_m$: **学生**网络在第m个位置的输出。因为输入被遮住了，所以这个输出通常是一个特殊的、可学习的 `[MASK]` 标志位对应的输出向量。
                *   $h_{\theta}$: 一个简单的神经网络（通常是一个MLP），被称为**预测头 (projection head)**。它的作用是把学生输出的 $p'_m$ 变换到和老师输出的 $p_m$ 相同的特征空间。
                *   $h_{\theta}(p'_m) - p_m$: 计算学生**预测的特征**和老师**真实的特征**之间的差异向量。
                *   $\| ... \|^2$: 计算这个差异向量的**L2范数的平方**，也就是**均方误差 (Mean Squared Error, MSE)**。这个值越小，说明学生的预测越准确。
                *   $\frac{1}{M} \sum_{m=1}^{M} ...$: 对所有被遮住的块的预测误差求平均。
            
            *   **总结：** $L_{iBOT}$ 强迫学生模型学习图像内部的空间上下文关系。为了能准确地“脑补”出被遮住部分的内容，模型必须理解图像的结构和纹理。
        
    *   **$L_{KoLeo}$ (特征散布损失)**：
        
        *   **目标**：防止**模型坍塌 (collapse)**。所谓坍塌，就是模型偷懒，对所有输入都输出完全相同或非常相似的特征。这个损失函数鼓励模型输出的特征在特征空间里**尽可能地散布开**，保持多样性。
        *   **公式(3)**: $L_{KoLeo} = -\frac{1}{B} \sum_{i=1}^{B} \log(\rho_i + \epsilon)$
            *   对于一批(batch)中的每一个特征$z_i$，计算它与最近邻特征$z_j$的距离$\rho_i$。通过最大化这个距离的对数（即最小化其相反数），来迫使所有特征互相推开。
        
    *   **$L_{contrast}$ (对比损失)**:
        *   **目标**：这是**Φeat的核心创新之一**，是实现“物理增强”的关键！
        *   **工作原理**：在一个训练批次(batch)中，我们会混合多种不同材质的渲染图。这个损失函数要求模型做到：
            
            1.  **拉近 (Pull)**：把所有**相同材质**的视图（比如“青铜水龙头”和“青铜雕塑”的所有裁剪图）在特征空间中的距离拉得尽可能近。
            2.  **推开 (Push)**：同时，把**不同材质**的视图（比如“青铜”和“橡木”）在特征空间中的距离推得尽可能远。
        *   **公式(5)**: $L_{contrast} = -\frac{1}{N} \sum_{i=1}^{N} \log \frac{\sum_{j \in P(i)} \exp(\text{sim}(z_i, z_j)/\tau)}{\sum_{k \neq i} \exp(\text{sim}(z_i, z_k)/\tau)}$
            
            *   这是一个经典的**InfoNCE对比损失**。对于一个样本$z_i$（锚点），分子是它与所有正样本（$P(i)$，即同材质的视图）的相似度之和。分母是它与所有其他样本（包括正样本和负样本）的相似度之-和。通过最大化这个比值，就实现了“拉近正样本，推开负样本”的目标。
            
        *   **直观思想：**
            想象在一个派对上，有来自不同家庭的人。你的任务是让**同一家人（相同材质）**都站到一起，抱成一团；同时让**不同家庭的人（不同材质）**互相推开，保持距离。
            
            **符号拆解：**
            
            *   $N$: 一个训练批次 (batch) 中的样本总数。
            *   $z_i$: 第 $i$ 个样本的特征向量，我们称之为**锚点 (anchor)**。
            *   $z_j$: 另一个样本的特征向量。
            *   $P(i)$: 一个集合，包含了所有与锚点 $z_i$ **同属于一个“家庭”（即同一材质）** 的样本，我们称之为**正样本 (positives)**。
            *   $\text{sim}(z_i, z_j)$: 计算两个特征向量 $z_i$ 和 $z_j$ 的**相似度**，通常使用**余弦相似度 (cosine similarity)**。值域为[-1, 1]，值越大表示越相似。
            *   $\tau$: 一个**温度超参数 (temperature)**。它用来调节相似度分数的分布。$\tau$越小，分布越“尖锐”，模型会更关注区分那些最难分的样本。
            *   $\exp(.../\tau)$: 将相似度分数通过指数函数映射，使其变为正数，并且放大相似度之间的差异。
            *   **分子** $\sum_{j \in P(i)} \exp(\text{sim}(z_i, z_j)/\tau)$: 计算锚点与**所有正样本**的相似度之和。我们希望这个值越大越好。
            *   **分母** $\sum_{k=1, k \neq i}^{N} \exp(\text{sim}(z_i, z_k)/\tau)$: 计算锚点与**批次内所有其他样本**（包括正样本和负样本）的相似度之和。
            *   **整个分数**：这个分数的含义是“锚点与正样本的相似度”占“锚点与所有样本总相似度”的比例。我们希望这个比例尽可能大，理想情况下接近1。
            *   $\log(...)$: 取对数。
            *   $-\frac{1}{N} \sum_{i=1}^{N} ...$: 对批次内所有样本作为锚点的损失求平均，并加负号，转换成最小化问题。
            
            **总结：** $L_{contrast}$ 通过最大化锚点与正样本的互信息（直观上就是让它们相似度占比最高），在特征空间中形成了**“类内紧凑，类间分离”**的良好结构。这里的“类”不是语义类别，而是由Φeat定义的“物理材质类别”。
        
    *   **$L_{Gram}$ (结构损失)**:
        *   **目标**：除了特征本身，这个损失还关注特征之间的**二阶关系**，即结构信息。它计算图像块特征之间的相关性矩阵（格拉姆矩阵），并要求学生的这个结构信息和另一个更稳定的老师（Gram Teacher，一个参数被冻结的老师网络）保持一致。这有助于学习到更稳定的纹理结构。
        *   **公式(4)**: $L_{Gram} = \frac{1}{N_p^2} \vert P_S^T P_S - P_G^T P_G \vert_F^2$
            *   **白话解释**：$P_S$和$P_G$分别是学生和Gram老师输出的图像块特征矩阵。$P^T P$计算的就是格拉姆矩阵。这个公式要求两个矩阵的差异（用弗罗贝尼乌斯范数的平方来衡量）最小。
    
5.  **总损失与模型更新**
    
    *   最后，将以上所有损失函数加权求和，得到总损失$L_{total} = L_{image} + \lambda_p L_{iBOT} + \lambda_k L_{KoLeo} + \lambda_g L_{Gram} + \lambda_c L_{contrast}$。
    *   根据这个总损失，使用优化器（如AdamW）来更新**学生网络**的参数。
    *   然后，再用EMA的方式，缓慢地更新**老师网络**的参数。
    *   重复以上过程成千上万次。



## 实验结果解析

#### 1. 核心能力验证：材质挑选 (Material Selection)

这个任务是检验Φeat核心能力的最直接方式。

*   **任务描述**：给定一张包含多种物体的图片，用户在这张图上**点一下**（选择一个查询块，query patch），模型需要把图中所有**具有相同材质**的区域都高亮出来。
*   **如何评估**：将模型输出的高亮区域（一个二值掩码）与人工标注的“标准答案”进行比较，计算**IoU**（交并比）和**F1分数**等指标。分数越高，说明挑选得越准。
*   **对比模型**：DINOv2, DINOv3（当时最强的自监督模型）。
*   **实验结果**：
    <img src="https://michaelyeung2004.github.io/assets/images/photos/image-20260120141230477.png" alt="image-20260120141230477" style="zoom:50%;" />
    *   **Φeat取得了压倒性的胜利**。它的IoU达到了0.776，F1分数达到了0.860，远超DINOv3的0.599和0.724。
    *   这证明Φeat确实学会了**忽略语义和物体边界，转而关注材质本身**。当用户点击一个木质的椅子腿时，DINO模型可能会把整个椅子都选出来（因为它认的是“椅子”），而Φeat则会准确地只选出木质部分，甚至还能把背景里另一个木质的桌子也一并选出来。

#### 2. 定性分析：可视化特征相似性与分割

![image-20260120141815936](https://michaelyeung2004.github.io/assets/images/photos/image-20260120141815936.png)

*   **左侧：块相似性图 (Patch-wise similarity)**
    *   **图上画了什么？** 图中用一个红十字标记了查询点。热力图显示了图中其他所有像素块的特征，与这个查询点的特征的**余弦相似度**。越亮表示越相似。
    *   **DINOv2/v3的表现**：当查询点在剪刀的金属部分时，DINO模型的高亮区域也基本集中在剪刀这个物体上。高亮区域随着与查询点的空间距离变远而衰减。这说明它的特征同时编码了**语义（“这是剪刀”）和空间位置**。
    *   **Φeat的表现**：Φeat的结果令人惊叹。当查询点在剪刀的金属部分时，它不仅把剪刀的两个金属叶片都高亮了，甚至把背景里一个毫不相干的金属罐子也以同样高的亮度高亮了出来！而剪刀的塑料手柄部分则是完全黑暗的。
    *   **结论**：这幅图完美地可视化了Φeat的核心能力——它学到的特征真正做到了**跨越物体、跨越空间**，只根据**物理材质**来建立关联。

*   **右侧：无监督分割 (Unsupervised segmentation)**
    *   **图上画了什么？** 这是将模型输出的像素块特征直接进行K-Means聚类的结果。颜色相同的区域表示被模型认为是“一类”。
    *   **DINOv2/v3的表现**：它们的分割结果更像是**语义分割**或**实例分割**。比如，它们会把整个剪刀分成一类（或金属和手柄两类），把金属罐子分成另一类。
    *   **Φeat的表现**：Φeat的分割结果则完全是**材质分割**。它会把剪刀的金属部分和金属罐子分成同一类（比如红色），而把剪-刀的塑料手柄分成另一类（比如蓝色），背景则是第三类。
    *   **结论**：再次证明Φeat的特征空间是按照物理材质来组织的，而不是语义类别。

#### **3. 特征可分离性与鲁棒性评估 (k-NN & Robustness)**

前面的实验证明Φeat能“识别”材质，但这些特征的“质量”如何呢？

*   **特征可分离性 (k-NN Classification, 见论文表2)**
    *   **任务**：作者用一个包含16种材质类别（木头、金属、织物等）的合成数据集进行测试。将所有图片编码成特征向量，然后用k-NN（k近邻）算法进行分类。分类准确率越高，说明**同类材质的特征在空间中聚集得越紧密，不同类材质的特征分得越开**。
    *   **结果**：Φeat的Top-1准确率达到了**0.643**，显著高于DINOv3的0.600和CLIP的0.552。这表明Φeat学到的材质特征不仅有效，而且“组织良好”，具有很好的区分度。

*   **鲁棒性 (Robustness, 见论文表3)**
    *   **任务**：这个实验非常巧妙。对于同一种材质，他们用**不同的光照**或**不同的几何形状**去渲染，然后比较模型对这些不同版本图片的预测结果是否一致。用“汉明距离”来衡量不一致性，**数值越低，说明模型越鲁棒**。
    *   **结果**：无论是在光照变化还是几何形状变化上，Φeat的汉明距离都是**最低的**。
    *   **结论**：这无可辩驳地证明了Φeat成功实现了其核心目标——学习到了**对光照和宏观几何形状都不变的 (invariant) 材质特征**。

#### **4. 消融实验 (Ablation Study, 见论文表4)**

这是科研论文的常规操作，目的是搞清楚模型里每个“新加的零件”是否真的有用。

*   **实验设计**：他们从一个基础的DINOv3模型开始，逐步添加Φeat的创新点（比如多渲染图监督、对比损失项），然后观察各项指标的变化。
*   **关键发现**：
    *   只用单张渲染图进行训练，虽然能提升“材质挑选”能力，但会严重损害模型的“分类”能力（k-NN准确率大幅下降）。这说明模型变得“只见树木，不见森林”了。
    *   加入多渲染图的监督后，分类能力有所恢复，说明模型开始学习不变性。
    *   **只有当最后加入了关键的对比损失项 ($L_{contrast}$) 后，模型才在所有任务上都取得了最佳性能**。
*   **结论**：这个实验证明了Φeat的成功并非偶然，而是其精心设计的**对比损失**与**物理增强**策略协同作用的结果。对比损失是强迫模型形成紧凑的、可分离的材质特征簇的关键。
