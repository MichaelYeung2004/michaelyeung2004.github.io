---
layout: blog
title: 'Φeat: 基于物理的特征表示'
date: 2026-01-20
tags:
  - deep learning
  - Text-to-Video Generation
  - paper

---

NewtonGen关心的是物体**如何运动**，而Φeat关心的是物体**由什么构成**。

## Background

现在的AI视觉模型（比如CLIP, DINO等）就像一个知识渊博但有点脸盲的图书管理员。你给它看一张照片，里面有一辆红色的金属跑车和一只红色的塑料玩具鸭。

你指着跑车的金属车门问它：“**这张图里，还有哪些东西和这个很像？**”

这个“图书管理员”AI可能会：
*   **语义关联 (Semantic Association)**：它会把跑车的其他部分，比如轮胎、车窗都找出来。因为它知道这些都属于“车”这个物体。
*   **颜色关联 (Color Association)**：它可能会把那只红色的塑料玩具鸭也圈出来，因为它俩都是红色的。

但你真正想问的可能是：“**这张图里，还有哪些东西是‘金属’的？**” 这个AI很可能就答不上来了。它可能无法理解，一把金属钥匙、一个不锈钢锅和这扇车门，在“材质”这个层面上是“同类”。

**这就是论文要解决的核心问题：**
现有的自监督学习模型非常擅长学习**语义 (Semantics)**，也就是“**这个东西是什么？**”（是车、是鸭子）。但它们很难分清语义和物理属性，导致它们不理解**物理特性 (Physical Properties)**，也就是“**这个东西是什么构成的？**”（是金属、是塑料、是木头）。

这种能力的缺失，在很多领域是致命的，比如：
*   **机器人**：机器人需要知道一个物体是软是硬，是光滑还是粗糙，才能决定用多大的力去抓取。
*   **自动驾驶**：需要判断路面是干燥沥青、是湿滑路面还是结冰，这都和材质有关。
*   **三维重建/AR**：要让虚拟物体真实地融入现实场景，就必须知道现实物体的材质，才能正确模拟光线反射。

## Φeat的核心思想

### 强迫AI学会“透过现象看本质”

为了教会AI理解材质，Φeat提出了一个极其巧妙的训练策略。这个策略的核心思想是改变“什么东西算作相似”的定义。

*   **传统AI的训练方式 (Photometric Augmentation)**：
    *   拿一张猫的照片。
    *   对这张照片进行各种“化妆”：裁剪一下、旋转一下、调亮一点、变个色调。
    *   然后告诉模型：“听着，所有这些‘化了妆’的照片，**本质上都是同一张图**，你要学会认出它们是同类。”
    *   这种方法教会了模型忽略光照、角度等表面变化，去识别“猫”这个核心语义。

*   **Φeat的训练方式 (Physical Augmentation)**：
    *   Φeat说：“我们来玩个更高级的游戏。”
    *   它首先获取一种**数字化的材质**，比如“橡木”的材质数据（包含了颜色、纹理、粗糙度、反射率等信息）。
    *   然后，它用计算机图形学技术，将这种“橡木”材质分别渲染（“贴”）到**完全不同的物体上**，并且使用**完全不同的光照环境**。
    *   于是，它得到了两张看起来毫不相干的图片：
        1.  一张是**阳光下的一把橡木椅子**。
        2.  另一张是**室内灯光下的一个橡木球**。
    *   然后Φeat告诉模型：“听着，这两张图，一张是椅子，一张是球，形状、光照、背景都不同，但你要学会认识到，它们在**物理本质上是同类**，因为它们都是‘橡木’！”

通过进行成千上万次这样的训练，模型的大脑就会被强行“改造”。它会逐渐意识到，不能只看物体的形状（椅子 vs. 球）或者场景（室外 vs. 室内），而必须学会**提取出那种独立于形状和光照之外的、更为根本的“材质”特征**。

### 训练数据

为了实现这种“物理增强”的训练，Φeat需要海量的、有精确材质标签的数据。在现实世界中收集这样的数据是不可能的。

因此，作者采用了**完全基于合成数据**的方法：
1.  他们使用了一个庞大的专业材质库（Adobe Substance 3D Assets），里面有上万种定义好的数字材质。
2.  他们创建了一系列3D模型作为“衣架”。
3.  他们使用多种HDRI环境贴图作为“灯光”。
4.  通过一个自动化流程，将不同的材质随机“穿”在不同的“衣架”上，并用不同的“灯光”去“拍摄”，最终生成了约一百万张高质量的渲染图片。

这些合成图片是Φeat训练的基石，因为它们提供了一个完美的实验环境：材质这个变量可以被精确控制，而其他所有变量（形状、光照、视角）都可以尽情地改变。



## Φeat的训练流程与核心技术

Φeat的训练框架是在一个非常成功的自监督学习模型 **DINO** 的基础上改进而来的。因此，要理解Φeat，我们首先要简单了解一下DINO的“师徒”模式。

### DINO的“师徒传授”模式 (Teacher-Student)

DINO框架里有两个模型，一个叫**老师 (Teacher)**，一个叫**学生 (Student)**。

*   **学生 (Student)**：是一个正常的、需要通过训练不断学习和更新参数的模型。
*   **老师 (Teacher)**：它的网络结构和学生**一模一样**，但它的参数**不是**通过常规的梯度下降来更新的。相反，它的参数是由“学生”的参数经过**指数移动平均 (Exponential Moving Average, EMA)** 缓慢更新而来。你可以把它想象成一个“更稳定、更有经验”版本的学生。
*   **学习过程**：
    1.  给“师徒”二人看同一张图片的不同“化妆”版本（比如一张裁剪过的，一张调亮过的）。
    2.  学生需要努力地去预测老师对于这张图片的“看法”（即老师网络输出的特征）。
    3.  通过不断缩小自己和老师“看法”的差距，学生就能学到对各种“化妆”都不变的稳定特征。

Φeat沿用了这种强大的师徒模式，但把“化妆”的方式从简单的图像处理，换成了我们上次讲的“物理增强”。

### Φeat的训练流程详解

![image-20260120100754085](https://michaelyeung2004.github.io\assets\images\photos\image-20260120100754085.png)

1.  **数据准备 (Data Preparation)**：
    *   首先，程序会随机选择**一种材质**，比如“拉丝青铜”。
    *   然后，用这种材质渲染**两张完全不同的图片**。例如：
        *   **视图1 (View 1)**：阳光下一个“拉丝青铜”做的水龙头。
        *   **视图2 (View 2)**：室内灯光下一个“拉丝青铜”做的雕塑。
    *   这两张图就是我们这次训练的核心输入，它们共享物理本质，但外观迥异。

2.  **多视角裁剪 (Multi-crop Strategy)**：
    *   对于视图1和视图2，分别进行“多视角裁剪”，得到：
        *   **2个全局视图 (Global crops)**：裁剪尺寸较大，保留了图像的整体结构。
        *   **多个局部视图 (Local crops)**：裁剪尺寸较小，关注材质的细节纹理。
    *   这样做的好处是让模型既能看到全局信息，又能关注局部细节。

3.  **师徒网络处理**：
    *   **学生网络 (Student)**：接收**所有的**裁剪视图（全局和局部）。
    *   **老师网络 (Teacher)**：为了节省计算量，它**只接收全局视图**。

4.  **计算损失函数 (The Core Objectives)**
    Φeat的强大之处在于它组合了多个损失函数，从不同角度来约束模型的学习。我们来逐一解析这些损失函数，它们是Φeat成功的关键。

    *   **$L_{image}$ (图像级损失)**：
        *   **目标**：让学生对一张图片（比如“青铜水龙头”）的看法，和老师对同一张图片的看法保持一致。这是**DINO的经典损失**，确保模型学习到对裁剪、缩放等操作的不变性。
        *   **公式(1)**: $L_{image} = -\frac{1}{|V_t||V_s|} \sum_{v_t \in V_t} \sum_{v_s \in V_s} \sum_{k=1}^{K} q_k(v_t) \log p_k(v_s)$
            *   **白话解释**：这是一个标准的**交叉熵损失**。$p_k(v_s)$是学生对某个视图的分类概率（把它归为K个类别中的哪一类），$q_k(v_t)$是老师给出的“标准答案”概率。目标就是让学生的$p$尽可能接近老师的$q$。

    *   **$L_{iBOT}$ (图像块级损失)**：
        *   **目标**：这个损失关注更细节的层面。它会随机地**遮住 (mask)** 学生输入图片的一部分图像块（patch），然后要求学生去**预测**老师网络看到的、**未被遮住的**对应图像块的特征。
        *   **公式(2)**: $L_{iBOT} = \frac{1}{M} \sum_{m=1}^{M} \| h_{\theta}(p'_m) - p_m \|^2$
            *   **白话解释**：$p'_m$是学生看到的被遮住的图像块，$p_m$是老师看到的未被遮住的对应图像块的特征。学生需要通过一个预测头$h_{\theta}$，把自己的信息恢复出来，并让恢复结果和老师的特征尽可能接近（欧氏距离的平方最小）。这强迫模型学习图像的内部上下文关系。

    *   **$L_{KoLeo}$ (特征散布损失)**：
        *   **目标**：防止**模型坍塌 (collapse)**。所谓坍塌，就是模型偷懒，对所有输入都输出完全相同或非常相似的特征。这个损失函数鼓励模型输出的特征在特征空间里**尽可能地散布开**，保持多样性。
        *   **公式(3)**: $L_{KoLeo} = -\frac{1}{B} \sum_{i=1}^{B} \log(\rho_i + \epsilon)$
            *   **白话解释**：对于一批(batch)中的每一个特征$z_i$，计算它与最近邻特征$z_j$的距离$\rho_i$。通过最大化这个距离的对数（即最小化其相反数），来迫使所有特征互相推开。

    *   **$L_{contrast}$ (对比损失)**:
        *   **目标**：这是**Φeat的核心创新之一**，是实现“物理增强”的关键！
        *   **工作原理**：在一个训练批次(batch)中，我们会混合多种不同材质的渲染图。这个损失函数要求模型做到：
            1.  **拉近 (Pull)**：把所有**相同材质**的视图（比如“青铜水龙头”和“青铜雕塑”的所有裁剪图）在特征空间中的距离拉得尽可能近。
            2.  **推开 (Push)**：同时，把**不同材质**的视图（比如“青铜”和“橡木”）在特征空间中的距离推得尽可能远。
        *   **公式(5)**: $L_{contrast} = -\frac{1}{N} \sum_{i=1}^{N} \log \frac{\sum_{j \in P(i)} \exp(\text{sim}(z_i, z_j)/\tau)}{\sum_{k \neq i} \exp(\text{sim}(z_i, z_k)/\tau)}$
            *   **白话解释**：这是一个经典的**InfoNCE对比损失**。对于一个样本$z_i$（锚点），分子是它与所有正样本（$P(i)$，即同材质的视图）的相似度之和。分母是它与所有其他样本（包括正样本和负样本）的相似度之-和。通过最大化这个比值，就实现了“拉近正样本，推开负样本”的目标。

    *   **$L_{Gram}$ (结构损失)**:
        *   **目标**：除了特征本身，这个损失还关注特征之间的**二阶关系**，即结构信息。它计算图像块特征之间的相关性矩阵（格拉姆矩阵），并要求学生的这个结构信息和另一个更稳定的老师（Gram Teacher，一个参数被冻结的老师网络）保持一致。这有助于学习到更稳定的纹理结构。
        *   **公式(4)**: $L_{Gram} = \frac{1}{N_p^2} \| P_S^T P_S - P_G^T P_G \|_F^2$
            *   **白话解释**：$P_S$和$P_G$分别是学生和Gram老师输出的图像块特征矩阵。$P^T P$计算的就是格拉姆矩阵。这个公式要求两个矩阵的差异（用弗罗贝尼乌斯范数的平方来衡量）最小。

5.  **总损失与模型更新**
    *   最后，将以上所有损失函数加权求和，得到**总损失 $L_{total}$** (公式6)。
    *   根据这个总损失，使用优化器（如AdamW）来更新**学生网络**的参数。
    *   然后，再用EMA的方式，缓慢地更新**老师网络**的参数。
    *   重复以上过程成千上万次。

---

**总结一下这次的内容：**
*   **科研名词解释**
    *   **师徒框架 (Teacher-Student Framework)**：一种自监督学习范式，让学生网络去学习一个更稳定的老师网络的输出。
    *   **指数移动平均 (EMA)**：一种更新策略，用于平滑地更新老师网络的权重，使其比学生网络更稳定。
    *   **对比损失 (Contrastive Loss)**：一种通过“拉近相似样本，推开不相似样本”来学习表征的损失函数。
*   **Φeat训练流程的核心**：
    1.  使用**物理增强**（同材质、不同形貌/光照）来创建训练对。
    2.  在DINO的**师徒框架**基础上，融合了**五种不同目标的损失函数**。
    3.  其中，$L_{image}$和$L_{iBOT}$继承自DINO，负责学习通用的视觉表征。
    4.  $L_{contrast}$是关键创新，它直接将“物理相似性”注入到学习过程中。
    5.  $L_{KoLeo}$和$L_{Gram}$作为辅助，分别保证了特征的多样性和结构的稳定性。

通过这个精心设计的组合拳，Φeat最终学到了一个对形状和光照不敏感，但对材质和微观结构高度敏感的强大视觉表征。在下一次回复中，我们将看看Φeat的**实验结果**，看看它在实际任务中表现如何，以及它学到的特征到底是什么样的。